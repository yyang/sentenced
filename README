What's new in this version
In this version, dynamic programming is leveraged to compute the differences between two words.
Definition: Distance(A, B), the distance between word A and B. Distance(A, B)=Distance(B, A)
a. Insert a character into A to get it more similar to B,  Distance(A, B)++;
b. Remove a character from A to get it more similar to B,  Distance(A, B)++;
c. change a character in A to get it more similar to B, Distance(A, B)++;
Definition: Similarity(A, B), the similarity of word A and B(0~1.0)
Similarity(A, B)=1-Distance(A, B)/MaxLength(A, B);
In Constant.h, threshold of the minimum similarity between two words is defined as macro of MINIMUM_WORD_SIMILARITY(default 0.6)
When the similarity of A and B is above this value,  the system suppose that A and B get the same meaning.


A.The package can be compiled by make command:
To compile it, use the command below:
make calc
make trainCorpus
make clean

B.To calculate the similarity between two sentences, use the command below:
./calc
Then input the sentence A and B 
sample:
[root@localhost nlp]# ./calc

Welcome to the sentences' similarity calculation module!
Input sentence A and B or type quit to exit~
Loading ...
Sentence A>Campus hunting is crucial for firms to attract talents
Sentence B>Talents are crucial for the development of firms

The similarity of 'Campus hunting is crucial for firms to attract talents' and 'Talents are crucial for the development of firms'
is 0.670192

C.There are three dictionaries
1. stop words.txt  
It's used to store the meaningless words in English, like 'hello', 'and', 'at'.etc.
If the stop word filter is open , the stop words in the sentence will be filtered.
By default this function is closed. You can turn it on by change the STOPWORD_CHECK in Constant.h to value 1.

2. symbols.txt
It's used to store the symbols in English,  like ', ',  '.', '!' and so on.

3. idf_dict.txt
It's used to store the most words' document frequency after training the corpus of tweets.1-3.data

D.To train the corpus ,  use the command below:
./trainCorpus train_data train_result
sample:
./trainCorpus tweets.2.data idf_dict_test.txt
Notice that the training process is time consuming which needs about 1 hour for each data file.


Performance:
a.For the similarity calcuation, normal CPU with memory of 1G is enough.
It takes about 18s to load all the dictionary into memory,  but the calculation process nearly costs 0 seconds.
b.For the training process,  it needs normal CPU with memory of 2G or more.(Perhaps 1G is OK , but it's slow)
During the process, it will print the line number being processed currently,  for each document of tweets.1-3.data, 
there are about 1000000 lines.
It takes about 1 hour to finish the training.Most time is cost by IO.

It's now version 2.0, you can contact me for more evaluation as soon as possible.
